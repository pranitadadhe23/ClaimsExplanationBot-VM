{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe0b3a09-36cb-43bb-976e-e471f4c01f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python-headless<5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: python-doctr[torch] in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from opencv-python-headless<5) (2.2.6)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (2.9.0)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (0.24.0)\n",
      "Requirement already satisfied: onnx<3.0.0,>=1.12.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (1.19.1)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (1.16.2)\n",
      "Requirement already satisfied: h5py<4.0.0,>=3.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (3.12.1)\n",
      "Requirement already satisfied: opencv-python<5.0.0,>=4.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (4.12.0.88)\n",
      "Requirement already satisfied: pypdfium2<5.0.0,>=4.11.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (4.30.0)\n",
      "Requirement already satisfied: pyclipper<2.0.0,>=1.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (1.3.0.post6)\n",
      "Requirement already satisfied: shapely<3.0.0,>=1.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (2.1.2)\n",
      "Requirement already satisfied: langdetect<2.0.0,>=1.0.9 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (3.14.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (0.36.0)\n",
      "Requirement already satisfied: Pillow>=9.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (11.1.0)\n",
      "Requirement already satisfied: defusedxml>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (0.7.1)\n",
      "Requirement already satisfied: anyascii>=0.3.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (0.3.3)\n",
      "Requirement already satisfied: validators>=0.18.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (0.35.0)\n",
      "Requirement already satisfied: tqdm>=4.30.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-doctr[torch]) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (4.12.2)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langdetect<2.0.0,>=1.0.9->python-doctr[torch]) (1.17.0)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from onnx<3.0.0,>=1.12.0->python-doctr[torch]) (5.29.3)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from onnx<3.0.0,>=1.12.0->python-doctr[torch]) (0.5.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->python-doctr[torch]) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->python-doctr[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->python-doctr[torch]) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.0.0->python-doctr[torch]) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch<3.0.0,>=2.0.0->python-doctr[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.30.0->python-doctr[torch]) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->torch<3.0.0,>=2.0.0->python-doctr[torch]) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (2025.10.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: python-doctr 1.0.0 does not provide the extra 'torch'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pdfplumber) (11.1.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (44.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.21)\n",
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: langdetect in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langdetect) (1.17.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.2.1)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.24.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: streamlit in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.45.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (5.5.1)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (2.2.6)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (5.29.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (19.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (4.0.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (6.5.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.31.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\asus\\anaconda3\\lib\\site-packages (11.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-doctr[torch] \"opencv-python-headless<5\"\n",
    "!pip install pdfplumber\n",
    "!pip install transformers\n",
    "!pip install langdetect\n",
    "!pip install sentencepiece\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install streamlit\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e669302-db6e-484b-bed9-9371a15a4adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.24.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef740973-3830-4d36-b4f5-75a1aad6a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4955b5fd-55f3-49aa-95e5-675a8fc2369d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading models...\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Models loaded successfully!\n",
      "\n",
      "\n",
      "üîπ Direct Text Test:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 160, but your input_length is only 91. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Detected Language: en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 160, but your input_length is only 99. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßæ **Customer-Friendly Explanation:**\n",
      "\n",
      "Claim ID 48290 was rejected because the health insurance policy had expired before the hospital admission date . The claim was submitted after the coverage period ended . The insured had last renewed the policy on 01-Apr-2022 with a grace period of 30 days .\n",
      "\n",
      "üîπ PDF Test:\n",
      "\n",
      "‚úÖ Extracted Raw Text (first 400 chars):\n",
      " Claim ID: 39561\n",
      "Patient Name: Priya Sharma\n",
      "Date of Admission: 21 Sept 2024\n",
      "Date of Discharge: 25 Sept 2024\n",
      "Hospital: Medico Hospital, Pune\n",
      "Diagnosis: Viral Fever\n",
      "Treatment: Hospitalization and IV Fluids\n",
      "Claim Amount: ‚Çπ32,000\n",
      "Approved Amount: ‚Çπ28,500\n",
      "Claim Status: APPROVED WITH REDUCTION\n",
      "Remarks: Non-medical expenses such as food and toiletries are not covered under the\n",
      "policy. \n",
      "\n",
      "üåê Detected Language: en\n",
      "üßæ **Customer-Friendly Explanation:**\n",
      "\n",
      "Priya Sharma was admitted to Medico Hospital, Pune with Viral Fever at 21 Sept 2024 . She was diagnosed with viral fever and was admitted with IV Fluids at the hospital . Food and toiletries such as toiletries are not covered under the policy .\n",
      "\n",
      "üîπ Image Test:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 160, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Extracted Raw Text (first 400 chars):\n",
      " Claim ID: 50923\n",
      "Patient Name: Amit Patel\n",
      "Date: 5 July 2024\n",
      "Hospital: CityCare Hospital, Nagpur\n",
      "Diagnosis: Dental Extraction\n",
      "Claim Status: DENIED\n",
      "Reason: Dental procedures are not covered under the current insurance plan. \n",
      "\n",
      "üåê Detected Language: en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 160, but your input_length is only 99. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßæ **Customer-Friendly Explanation:**\n",
      "\n",
      "Dental procedures are not covered under the current insurance plan . Claim ID: 50923 Patient Name: Amit Patel Date: 5 July 2024 Hospital: CityCare Hospital, Nagpur Diagnosis: Dental Extraction Claim Status: DENIED .\n",
      "\n",
      "üîπ Text File Test:\n",
      "\n",
      "‚úÖ Extracted Raw Text (first 400 chars):\n",
      " Claim ID: 48290  \n",
      "Patient Name: Ramesh Kumar  \n",
      "Date of Admission: 12 Aug 2024  \n",
      "Date of Discharge: 16 Aug 2024  \n",
      "Hospital Name: LifeCare Multispeciality Hospital  \n",
      "\n",
      "Diagnosis: Acute Appendicitis  \n",
      "Treatment: Appendectomy  \n",
      "\n",
      "Claim Amount Submitted: ‚Çπ58,000  \n",
      "Amount Approved: ‚Çπ0  \n",
      "Claim Status: REJECTED  \n",
      "\n",
      "Reason for Rejection: The health insurance policy expired on 10 Aug 2024, two days before admi \n",
      "\n",
      "üåê Detected Language: en\n",
      "üßæ **Customer-Friendly Explanation:**\n",
      "\n",
      "Claim ID: 48290 Patient Name: Ramesh Kumar Date of Admission: 12 Aug 2024 Date of Discharge: 16 Aug 2024 Hospital Name: LifeCare Multispeciality Hospital Diagnosis: Acute Appendicitis Treatment: Appendectomy Claim Amount Submitted: ‚Åπ58,000 Amount Approved: ‚Çπ0 Claim Status: REjected Reason for Rejection: The health insurance policy expired on 10 Aug 2024, two days before admission .\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üß© INSURANCE CLAIM SUMMARIZER ‚Äî FIXED FOR LONG REPORTS\n",
    "# ============================================================\n",
    "\n",
    "# ---------------------------\n",
    "# üì¶ Imports\n",
    "# ---------------------------\n",
    "import re\n",
    "import pdfplumber\n",
    "from doctr.io import DocumentFile\n",
    "from doctr.models import ocr_predictor\n",
    "from langdetect import detect\n",
    "from transformers import MarianTokenizer, MarianMTModel, pipeline\n",
    "\n",
    "# ---------------------------\n",
    "# ‚öôÔ∏è Load OCR + Summarization Models\n",
    "# ---------------------------\n",
    "print(\"‚è≥ Loading models...\")\n",
    "\n",
    "# DocTR OCR (CPU mode)\n",
    "ocr_model = ocr_predictor(pretrained=True)\n",
    "\n",
    "# üîÅ You can swap this model name if you want better quality (slower):\n",
    "# e.g. \"facebook/bart-large-cnn\"\n",
    "SUMMARIZATION_MODEL_NAME = \"sshleifer/distilbart-cnn-12-6\"\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=SUMMARIZATION_MODEL_NAME,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Models loaded successfully!\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üß† Utility Functions\n",
    "# ============================================================\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF using pdfplumber, fallback to DocTR OCR.\"\"\"\n",
    "    text = \"\"\n",
    "    # 1Ô∏è‚É£ Try digital text first\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "\n",
    "    if text.strip():\n",
    "        return text.strip()\n",
    "\n",
    "    # 2Ô∏è‚É£ Fallback: OCR with DocTR\n",
    "    print(\"üîç Using OCR for scanned PDF...\")\n",
    "    doc = DocumentFile.from_pdf(pdf_path)\n",
    "    result = ocr_model(doc)\n",
    "    # doctr's .render() returns a full-page text representation\n",
    "    return result.render()\n",
    "\n",
    "\n",
    "def extract_text_from_image(image_path: str) -> str:\n",
    "    \"\"\"Extract text from image using DocTR OCR.\"\"\"\n",
    "    doc = DocumentFile.from_images(image_path)\n",
    "    result = ocr_model(doc)\n",
    "    return result.render()\n",
    "\n",
    "\n",
    "def auto_detect_language(text: str) -> str:\n",
    "    \"\"\"Auto-detect language code from text.\"\"\"\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except Exception:\n",
    "        return \"en\"\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# üåê Translation (kept same stack, but safe)\n",
    "# ---------------------------\n",
    "\n",
    "def translate_text(text: str, src_lang: str, tgt_lang: str, max_chunk_chars: int = 1500) -> str:\n",
    "    \"\"\"\n",
    "    Generic translation using MarianMT (Helsinki-NLP models).\n",
    "    Now supports long texts by translating in chunks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_name = f\"Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}\"\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "        model = MarianMTModel.from_pretrained(model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Translation model for {src_lang}->{tgt_lang} not found ({e}). Using original text.\")\n",
    "        return text\n",
    "\n",
    "    # Normalize whitespace a bit\n",
    "    text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    # Chunk text to avoid 512-token truncation\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(start + max_chunk_chars, n)\n",
    "        # Try to break at the last period near the end of the window\n",
    "        split = text.rfind(\".\", start, end)\n",
    "        if split == -1 or split <= start + 100:  # no good period, or too early\n",
    "            split = end\n",
    "        else:\n",
    "            split += 1  # include the period\n",
    "        chunks.append(text[start:split].strip())\n",
    "        start = split\n",
    "\n",
    "    translated_chunks = []\n",
    "    for c in chunks:\n",
    "        if not c.strip():\n",
    "            continue\n",
    "        inputs = tokenizer(\n",
    "            c,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "        )\n",
    "        out_ids = model.generate(**inputs, max_length=512)\n",
    "        translated = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "        translated_chunks.append(translated.strip())\n",
    "\n",
    "    return \" \".join(translated_chunks).strip()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# üß† Long-text summarization\n",
    "# ---------------------------\n",
    "\n",
    "def _summarize_chunk(chunk: str, max_length: int = 160, min_length: int = 50) -> str:\n",
    "    \"\"\"Call HF summarizer safely on one chunk.\"\"\"\n",
    "    if not chunk.strip():\n",
    "        return \"\"\n",
    "    # Hugging Face pipeline expects text <= model max tokens. We approximate via chars.\n",
    "    result = summarizer(\n",
    "        chunk,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    return result[0][\"summary_text\"].strip()\n",
    "\n",
    "\n",
    "def summarize_long_text(\n",
    "    text: str,\n",
    "    max_chunk_chars: int = 2500,\n",
    "    chunk_summary_max_len: int = 160,\n",
    "    chunk_summary_min_len: int = 50,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Summarize very long claim reports by:\n",
    "      1. Cleaning + normalizing text\n",
    "      2. Splitting into overlapping chunks\n",
    "      3. Summarizing each chunk\n",
    "      4. Optionally summarizing the concatenated summaries again\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return \"‚ö†Ô∏è No text detected in the document.\"\n",
    "\n",
    "    # Basic normalization\n",
    "    text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "\n",
    "    if len(text) <= max_chunk_chars:\n",
    "        # Short enough: one-shot summarization\n",
    "        return _summarize_chunk(\n",
    "            text,\n",
    "            max_length=chunk_summary_max_len,\n",
    "            min_length=chunk_summary_min_len,\n",
    "        )\n",
    "\n",
    "    # 1Ô∏è‚É£ Split into chunks at sentence boundaries where possible\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(start + max_chunk_chars, n)\n",
    "        # Try to end at a period close to the limit\n",
    "        split = text.rfind(\".\", start, end)\n",
    "        if split == -1 or split <= start + 400:  # no good split; just hard cut\n",
    "            split = end\n",
    "        else:\n",
    "            split += 1  # include period\n",
    "        chunk = text[start:split].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start = split\n",
    "\n",
    "    # 2Ô∏è‚É£ Summarize each chunk\n",
    "    partial_summaries = []\n",
    "    for i, c in enumerate(chunks, start=1):\n",
    "        print(f\"üß© Summarizing chunk {i}/{len(chunks)} (len={len(c)} chars)\")\n",
    "        # If chunk is tiny, keep it as is\n",
    "        if len(c.split()) < 40:\n",
    "            partial_summaries.append(c)\n",
    "        else:\n",
    "            s = _summarize_chunk(\n",
    "                c,\n",
    "                max_length=chunk_summary_max_len,\n",
    "                min_length=chunk_summary_min_len,\n",
    "            )\n",
    "            partial_summaries.append(s)\n",
    "\n",
    "    # 3Ô∏è‚É£ Combine summaries; if still long, summarize again\n",
    "    combined = \" \".join(partial_summaries)\n",
    "    combined = re.sub(r\"\\s+\", \" \", combined).strip()\n",
    "\n",
    "    if len(combined) <= max_chunk_chars:\n",
    "        final = _summarize_chunk(\n",
    "            combined,\n",
    "            max_length=chunk_summary_max_len,\n",
    "            min_length=chunk_summary_min_len,\n",
    "        )\n",
    "        return final\n",
    "\n",
    "    # If still very long, just return concatenated summaries\n",
    "    return combined\n",
    "\n",
    "\n",
    "def summarize_claim(text: str) -> str:\n",
    "    \"\"\"Summarize insurance claim in customer-friendly English (long-text aware).\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"‚ö†Ô∏è No text detected in the document.\"\n",
    "\n",
    "    summary = summarize_long_text(text)\n",
    "    return summary.strip()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# üîó End-to-end pipelines\n",
    "# ---------------------------\n",
    "\n",
    "def process_claim_file(file_path: str, back_translate: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Main pipeline:\n",
    "    Extract ‚Üí Detect Lang ‚Üí Translate ‚Üí Summarize ‚Üí Back-translate.\n",
    "    Handles long reports via chunked translation & summarization.\n",
    "    \"\"\"\n",
    "    # 1Ô∏è‚É£ Extract text based on file type\n",
    "    file_lower = file_path.lower()\n",
    "    if file_lower.endswith(\".pdf\"):\n",
    "        raw_text = extract_text_from_pdf(file_path)\n",
    "    elif file_lower.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        raw_text = extract_text_from_image(file_path)\n",
    "    elif file_lower.endswith(\".txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            raw_text = f.read()\n",
    "    else:\n",
    "        return \"‚ö†Ô∏è Unsupported file type.\"\n",
    "\n",
    "    if not raw_text.strip():\n",
    "        return \"‚ö†Ô∏è No readable text found in the document.\"\n",
    "\n",
    "    print(\"\\n‚úÖ Extracted Raw Text (first 400 chars):\\n\", raw_text[:400], \"\\n\")\n",
    "\n",
    "    # 2Ô∏è‚É£ Detect language automatically\n",
    "    src_lang = auto_detect_language(raw_text)\n",
    "    print(f\"üåê Detected Language: {src_lang}\")\n",
    "\n",
    "    # 3Ô∏è‚É£ Translate to English if not already English (chunked translation)\n",
    "    if src_lang != \"en\":\n",
    "        text_in_english = translate_text(raw_text, src_lang, \"en\")\n",
    "    else:\n",
    "        text_in_english = raw_text\n",
    "\n",
    "    # 4Ô∏è‚É£ Summarize claim (long-text aware)\n",
    "    summary_en = summarize_claim(text_in_english)\n",
    "\n",
    "    # 5Ô∏è‚É£ Optionally translate summary back to source language\n",
    "    if back_translate and src_lang != \"en\":\n",
    "        summary_final = translate_text(summary_en, \"en\", src_lang)\n",
    "    else:\n",
    "        summary_final = summary_en\n",
    "\n",
    "    return f\"üßæ **Customer-Friendly Explanation:**\\n\\n{summary_final}\"\n",
    "\n",
    "\n",
    "def process_claim_text(text: str, back_translate: bool = True) -> str:\n",
    "    \"\"\"Directly process a claim text input.\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"‚ö†Ô∏è Empty text.\"\n",
    "\n",
    "    src_lang = auto_detect_language(text)\n",
    "    print(f\"üåê Detected Language: {src_lang}\")\n",
    "\n",
    "    if src_lang != \"en\":\n",
    "        text_in_english = translate_text(text, src_lang, \"en\")\n",
    "    else:\n",
    "        text_in_english = text\n",
    "\n",
    "    summary_en = summarize_claim(text_in_english)\n",
    "\n",
    "    if back_translate and src_lang != \"en\":\n",
    "        summary_final = translate_text(summary_en, \"en\", src_lang)\n",
    "    else:\n",
    "        summary_final = summary_en\n",
    "\n",
    "    return f\"üßæ **Customer-Friendly Explanation:**\\n\\n{summary_final}\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# üß™ Example Tests\n",
    "# ============================================================\n",
    "\n",
    "claim_report = \"\"\"\n",
    "Claim ID 48290 was rejected because the health insurance policy had expired\n",
    "before the hospital admission date. The claim was submitted after the coverage\n",
    "period ended. The insured had last renewed the policy on 01-Apr-2022 with a\n",
    "grace period of 30 days; however, the hospitalization occurred on 15-May-2022\n",
    "after the expiry of the grace period. As per policy conditions, claims after\n",
    "expiry of coverage are not admissible.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüîπ Direct Text Test:\")\n",
    "print(process_claim_text(claim_report))\n",
    "\n",
    "print(\"\\nüîπ PDF Test:\")\n",
    "print(process_claim_file(\"test_files/sample_claim.pdf\"))\n",
    "\n",
    "print(\"\\nüîπ Image Test:\")\n",
    "print(process_claim_file(\"test_files/claim_image.png\"))\n",
    "\n",
    "print(\"\\nüîπ Text File Test:\")\n",
    "print(process_claim_file(\"test_files/claim_note.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cbece42-5e7b-4f20-a107-a766bf88fd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.45.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: Pillow in c:\\users\\asus\\anaconda3\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (5.5.1)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (2.2.6)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (5.29.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (19.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (4.0.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from streamlit) (6.5.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.31.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (44.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.21)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit transformers torch sentencepiece pytesseract pdfplumber Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52ff7c5d-bb07-47f8-9662-5ef2bf900253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sumy in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: python-docx in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: docopt<0.7,>=0.6.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sumy) (0.6.2)\n",
      "Requirement already satisfied: breadability>=0.1.20 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sumy) (0.1.20)\n",
      "Requirement already satisfied: requests>=2.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sumy) (2.32.3)\n",
      "Requirement already satisfied: pycountry>=18.2.23 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sumy) (24.6.1)\n",
      "Requirement already satisfied: nltk>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sumy) (3.9.1)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-docx) (5.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: chardet in c:\\users\\asus\\anaconda3\\lib\\site-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (2025.10.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from click->nltk>=3.0.2->sumy) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install sumy python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "778fa247-f9ff-43dd-8884-76e589083428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting claim_explainer_app_fixed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile claim_explainer_app_fixed.py\n",
    "# claim_explainer_app_fixed.py\n",
    "# ============================================================\n",
    "# ‚ö° Insurance Claim Explainer ‚Äî Streamlit + Gen AI Summary\n",
    "# ============================================================\n",
    "# - Extracts key fields (Claim ID, Status, Amounts, Reason, etc.)\n",
    "# - Generates a rule-based explanation\n",
    "# - PLUS an AI-generated, long-text-aware summary of the full report\n",
    "#\n",
    "# Run from terminal or Jupyter:\n",
    "#   streamlit run claim_explainer_app_fixed.py\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import streamlit as st\n",
    "import pdfplumber\n",
    "\n",
    "# ---------- Optional OCR (DocTR) ----------\n",
    "try:\n",
    "    from doctr.io import DocumentFile\n",
    "    from doctr.models import ocr_predictor\n",
    "    DOCTR_AVAILABLE = True\n",
    "except Exception:\n",
    "    DocumentFile = None\n",
    "    ocr_predictor = None\n",
    "    DOCTR_AVAILABLE = False\n",
    "\n",
    "# ---------- Gen AI imports ----------\n",
    "from langdetect import detect\n",
    "from transformers import pipeline, MarianTokenizer, MarianMTModel\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Streamlit page setup & styles\n",
    "# ------------------------------------------------------------\n",
    "st.set_page_config(page_title=\"Insurance Claim Explainer\", page_icon=\"üßæ\", layout=\"centered\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "<style>\n",
    ".app-title{text-align:center;font-size:36px;color:#123b5a;font-weight:800;margin-bottom:4px}\n",
    ".app-sub{text-align:center;color:#52606d;margin-bottom:20px}\n",
    ".card{background:#fff;border:1px solid #e6edf5;border-radius:16px;padding:20px;box-shadow:0 5px 16px rgba(0,0,0,0.05)}\n",
    ".result{background:#f7fafc;border:1px solid #e6edf5;border-radius:14px;padding:18px;margin-top:16px;white-space:pre-wrap;font-size:15px;color:#0f1a2d}\n",
    ".footer{text-align:center;color:#738093;font-size:12px;margin-top:20px}\n",
    ".section-title{font-weight:700;font-size:18px;margin-top:8px;margin-bottom:6px;color:#123b5a}\n",
    ".section-sub{font-weight:500;font-size:14px;margin-bottom:8px;color:#52606d}\n",
    "</style>\n",
    "\"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "st.markdown('<div class=\"app-title\">üßæ Claim Explanation</div>', unsafe_allow_html=True)\n",
    "st.markdown('<div class=\"app-sub\">Instant, clear summaries showing claim approval or rejection status.</div>', unsafe_allow_html=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OCR model loader (cached)\n",
    "# ------------------------------------------------------------\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def load_ocr_model():\n",
    "    if not DOCTR_AVAILABLE:\n",
    "        return None\n",
    "    try:\n",
    "        return ocr_predictor(pretrained=True)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "ocr_model = load_ocr_model()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Gen AI: summarizer loader (cached)\n",
    "# ------------------------------------------------------------\n",
    "SUMMARIZATION_MODEL_NAME = \"sshleifer/distilbart-cnn-12-6\"  # small, CPU-friendly\n",
    "\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def load_summarizer():\n",
    "    return pipeline(\"summarization\", model=SUMMARIZATION_MODEL_NAME)\n",
    "\n",
    "summarizer = load_summarizer()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Text cleaning and extraction helpers (robust)\n",
    "# ------------------------------------------------------------\n",
    "def clean_text(t: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize whitespace, remove repeated separator lines, and cut off\n",
    "    obvious footer/sign-off blocks to avoid footer bleed into fields.\n",
    "    \"\"\"\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    # Normalize newlines\n",
    "    t = t.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # Remove long separator lines (----- **** ____ etc.)\n",
    "    t = re.sub(r'(?m)^[\\-\\=_\\*]{3,}\\s*$', '\\n', t)\n",
    "\n",
    "    # Strip trailing whitespace per line\n",
    "    t = re.sub(r'[ \\t]+$', '', t, flags=re.M)\n",
    "\n",
    "    # Collapse many blank lines\n",
    "    t = re.sub(r'\\n{3,}', '\\n\\n', t)\n",
    "\n",
    "    # Cut off after common footer headers to avoid footer bleed into earlier fields\n",
    "    cutoff_headers = [\n",
    "        r'FINAL REMARKS', r'SIGN-?OFF', r'END OF REPORT', r'REVIEWED & SIGNED BY',\n",
    "        r'APPROVAL & FINANCIAL ASSESSMENT', r'SIGN-OFF ‚Äî CLAIMS DEPARTMENT', r'FINAL REMARKS:'\n",
    "    ]\n",
    "    for h in cutoff_headers:\n",
    "        m = re.search(fr'(?im)\\n{h}.*', t)\n",
    "        if m:\n",
    "            t = t[:m.start()].strip()\n",
    "            break\n",
    "\n",
    "    return t.strip()\n",
    "\n",
    "def _extract_section(txt: str, header_patterns, stop_patterns=None):\n",
    "    \"\"\"\n",
    "    Find a header (one of header_patterns) and extract the block that follows,\n",
    "    stopping at the nearest pattern in stop_patterns (if given). Handles:\n",
    "     - Header on its own line followed by a block\n",
    "     - Header and value on same line: 'Reason: some text'\n",
    "    Returns None if not found.\n",
    "    \"\"\"\n",
    "    if not txt:\n",
    "        return None\n",
    "\n",
    "    if isinstance(header_patterns, (list, tuple)):\n",
    "        header_rx = r'(?:' + r'|'.join(header_patterns) + r')'\n",
    "    else:\n",
    "        header_rx = r'(?:' + header_patterns + r')'\n",
    "\n",
    "    # Case 1: header on its own line, then block\n",
    "    m = re.search(fr'(?im){header_rx}\\s*[:\\-]?\\s*\\n', txt)\n",
    "    if m:\n",
    "        start = m.end()\n",
    "        end = len(txt)\n",
    "        if stop_patterns:\n",
    "            stops = stop_patterns if isinstance(stop_patterns, (list, tuple)) else [stop_patterns]\n",
    "            nearest = None\n",
    "            for s in stops:\n",
    "                ms = re.search(fr'(?im)\\n{s}\\s*[:\\-]?\\s*\\n', txt[start:])\n",
    "                if ms:\n",
    "                    end_idx = start + ms.start()\n",
    "                    if nearest is None or end_idx < nearest:\n",
    "                        nearest = end_idx\n",
    "            if nearest is not None:\n",
    "                end = nearest\n",
    "        section = txt[start:end].strip()\n",
    "        return section or None\n",
    "\n",
    "    # Case 2: header and value on same line\n",
    "    m2 = re.search(fr'(?im){header_rx}\\s*[:\\-]\\s*(.+)', txt)\n",
    "    if m2:\n",
    "        val = m2.group(1)\n",
    "        return val.strip() if isinstance(val, str) and val.strip() else None\n",
    "\n",
    "    return None\n",
    "\n",
    "def resolve_status(txt: str, info_status: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Gather all status mentions in document and return the last one\n",
    "    (assumes later mentions override earlier ones).\n",
    "    \"\"\"\n",
    "    if not txt:\n",
    "        return info_status\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    # explicit status lines\n",
    "    for m in re.finditer(r'(?im)(?:Claim\\s*Status|Status)\\s*[:\\-]\\s*([A-Za-z \\-]+)', txt):\n",
    "        val = m.group(1).strip()\n",
    "        if val:\n",
    "            candidates.append(val)\n",
    "\n",
    "    # \"Automatically updated status: REJECTED\" style\n",
    "    for m in re.finditer(r'(?im)(?:Automatically updated status|Automatically updated)\\s*[:\\-]?\\s*([A-Za-z \\-]+)', txt):\n",
    "        val = m.group(1).strip()\n",
    "        if val:\n",
    "            candidates.append(val)\n",
    "\n",
    "    # standalone keywords as they appear (in order)\n",
    "    for m in re.finditer(r'(?im)\\b(Approved with Reduction|Partially Approved|Approved|Denied|Rejected|Declined|Full Approval|FULL APPROVAL|APPROVED|DECLINED|DECLINED)\\b', txt):\n",
    "        candidates.append(m.group(1).strip())\n",
    "\n",
    "    if candidates:\n",
    "        return candidates[-1]\n",
    "    return info_status\n",
    "\n",
    "def extract_info(txt: str) -> dict:\n",
    "    \"\"\"\n",
    "    Safely extract fields from cleaned text. Returns a dict with keys:\n",
    "    claim_id, policy_number, patient, age, gender, uhid, hospital, hospital_city,\n",
    "    admission_date, discharge_date, claim_amount, approved_amount, status, reason\n",
    "    \"\"\"\n",
    "    info = {}\n",
    "    t = txt or \"\"\n",
    "\n",
    "    def single_line(pat):\n",
    "        m = re.search(fr'(?im)(?:{pat})\\s*[:\\-]\\s*(.+)', t)\n",
    "        if not m:\n",
    "            return None\n",
    "        v = m.group(1)\n",
    "        return v.strip() if isinstance(v, str) and v.strip() else None\n",
    "\n",
    "    # IDs and simple fields\n",
    "    info['claim_id'] = single_line(r'Claim\\s*ID|Claim\\s*No|Claim\\s*Number|CLAIM\\s*:|CLAIM\\s*:')\n",
    "    info['policy_number'] = single_line(r'Policy\\s*Number|Policy\\s*No|POLICY\\s*NO|Policy\\s*:')\n",
    "    info['patient'] = single_line(r'Patient\\s*Name|PATIENT\\s*NAME|Name\\b')\n",
    "    # Age/Gender combined or separate\n",
    "    ag = single_line(r'Age\\s*/\\s*Gender|Age\\s*\\(Years\\)\\s*/\\s*Gender|Age/Gender|AGE/GENDER|Age[:]')\n",
    "    if ag:\n",
    "        m = re.search(r'(\\d{1,3})\\s*/\\s*([A-Za-z]+)', ag)\n",
    "        if m:\n",
    "            info['age'] = m.group(1)\n",
    "            info['gender'] = m.group(2)\n",
    "        else:\n",
    "            parts = [p.strip() for p in re.split(r'[/,|]', ag) if p.strip()]\n",
    "            for p in parts:\n",
    "                if re.match(r'^\\d{1,3}$', p):\n",
    "                    info['age'] = p\n",
    "                elif p.isalpha():\n",
    "                    info['gender'] = p\n",
    "\n",
    "    info['uhid'] = single_line(r'UHID|UHID[: ]')\n",
    "    info['hospital'] = single_line(r'Hospital\\s*Name|HOSPITAL|Hospital[:]')\n",
    "    info['hospital_city'] = single_line(r'Hospital\\s*City|HOSPITAL\\s*LOCATION|City\\s*:')\n",
    "    info['admission_date'] = single_line(r'Admission\\s*Date|Admit[:]')\n",
    "    info['discharge_date'] = single_line(r'Discharge\\s*Date|Discharge[:]')\n",
    "\n",
    "    # Claim amounts - try explicit patterns, then looser paragraph patterns\n",
    "    m = re.search(r'(?im)Claim\\s*Amount\\s*(?:Submitted)?\\s*[:\\-]?\\s*[‚ÇπINR\\s]*([0-9,]+(?:\\.\\d+)?)', t)\n",
    "    if m:\n",
    "        info['claim_amount'] = m.group(1).strip()\n",
    "    else:\n",
    "        m2 = re.search(r'(?im)(?:total (?:hospital )?expenses|total billed|total claimed|expenses came to|expenses were|Bill paid by patient|expenses paid)\\s*(?:[:\\-]?)\\s*[‚ÇπINR\\s]*([0-9,]+(?:\\.\\d+)?)', t)\n",
    "        if m2:\n",
    "            info['claim_amount'] = m2.group(1).strip()\n",
    "\n",
    "    m3 = re.search(r'(?im)Approved\\s*Amount\\s*[:\\-]?\\s*[‚ÇπINR\\s]*([0-9,]+(?:\\.\\d+)?)', t)\n",
    "    if m3:\n",
    "        info['approved_amount'] = m3.group(1).strip()\n",
    "\n",
    "    # Status: combine inline and resolved status\n",
    "    inline_status = single_line(r'Claim\\s*Status|Status')\n",
    "    resolved = resolve_status(t, inline_status)\n",
    "    info['status'] = resolved\n",
    "\n",
    "    # Reason: try section-aware extraction first, then fallback patterns\n",
    "    reason_section = _extract_section(\n",
    "        t,\n",
    "        header_patterns=[r'REASON\\s*FOR\\s*REJECTION', r'REASON\\s*FOR\\s*REJECTIONS?', r'Reason', r'Reason\\s*:'],\n",
    "        stop_patterns=[r'FINAL\\s+REMARKS', r'SIGN-?OFF', r'APPROVAL\\s*&\\s*FINANCIAL', r'HOSPITAL\\s*BILL\\s*SUMMARY', r'FINAL\\s+REMARKS:']\n",
    "    )\n",
    "    if reason_section:\n",
    "        reason = re.sub(r'\\s+', ' ', reason_section).strip()\n",
    "        info['reason'] = (reason[:800] + '...') if len(reason) > 800 else reason\n",
    "    else:\n",
    "        m = re.search(r'(?im)(policy (?:was )?inactive.*?grace period.*?\\.?)', t)\n",
    "        if m:\n",
    "            info['reason'] = m.group(1).strip()\n",
    "        else:\n",
    "            m2 = re.search(r'(?im)Reason\\s*[:\\-]?\\s*(.+)', t)\n",
    "            if m2 and m2.group(1):\n",
    "                info['reason'] = m2.group(1).strip()\n",
    "\n",
    "    return info\n",
    "\n",
    "def quick_summary(txt: str, info: dict) -> str:\n",
    "    \"\"\"\n",
    "    Build a compact, human-friendly summary using resolved status and trimmed fields.\n",
    "    \"\"\"\n",
    "    status_text = (info.get('status') or '') or ''\n",
    "    stx = (status_text or '').lower()\n",
    "\n",
    "    decision = \"Unknown\"\n",
    "    if stx:\n",
    "        if 'approved' in stx and ('reduction' in stx or 'partial' in stx):\n",
    "            decision = \"Approved with Reduction\"\n",
    "        elif 'approved' in stx or 'full approval' in stx or ('full' in stx and 'approval' in stx):\n",
    "            decision = \"Approved\"\n",
    "        elif 'rejected' in stx or 'denied' in stx or 'declined' in stx or 'decline' in stx:\n",
    "            decision = \"Rejected\"\n",
    "        else:\n",
    "            if 'reject' in stx or 'deni' in stx:\n",
    "                decision = \"Rejected\"\n",
    "            elif 'approv' in stx:\n",
    "                decision = \"Approved\"\n",
    "\n",
    "    # fallback: search document for last-standing status if still unknown\n",
    "    if decision == \"Unknown\":\n",
    "        final_status = resolve_status(txt, None)\n",
    "        if final_status:\n",
    "            fs = final_status.lower()\n",
    "            if 'reject' in fs or 'deni' in fs:\n",
    "                decision = \"Rejected\"\n",
    "            elif 'approv' in fs:\n",
    "                decision = \"Approved\"\n",
    "\n",
    "    lines = [f\"Decision: {decision}\"]\n",
    "    if info.get('claim_id'):\n",
    "        lines.append(f\"Claim ID: {info['claim_id']}\")\n",
    "    if info.get('policy_number'):\n",
    "        lines.append(f\"Policy: {info['policy_number']}\")\n",
    "\n",
    "    # Patient\n",
    "    patient_parts = []\n",
    "    if info.get('patient'):\n",
    "        patient_parts.append(info['patient'])\n",
    "    if info.get('age'):\n",
    "        patient_parts.append(f\"Age: {info['age']}\")\n",
    "    if info.get('gender'):\n",
    "        patient_parts.append(f\"Gender: {info['gender']}\")\n",
    "    if patient_parts:\n",
    "        lines.append(\"Patient: \" + \" | \".join(patient_parts))\n",
    "\n",
    "    # Hospital\n",
    "    if info.get('hospital'):\n",
    "        hosp = info['hospital']\n",
    "        if info.get('hospital_city'):\n",
    "            hosp = f\"{hosp} ‚Äî {info['hospital_city']}\"\n",
    "        lines.append(f\"Hospital: {hosp}\")\n",
    "\n",
    "    # Amounts\n",
    "    if info.get('claim_amount'):\n",
    "        lines.append(f\"Claimed: ‚Çπ{info['claim_amount']}\")\n",
    "    if info.get('approved_amount'):\n",
    "        lines.append(f\"Approved: ‚Çπ{info['approved_amount']}\")\n",
    "\n",
    "    # Reason (trimmed)\n",
    "    if info.get('reason'):\n",
    "        reason = info['reason'].strip()\n",
    "        if len(reason) > 500:\n",
    "            reason = reason[:500].rstrip() + \"...\"\n",
    "        lines.append(f\"Reason: {reason}\")\n",
    "\n",
    "    # Outcome guidance\n",
    "    if decision.startswith(\"Approved\"):\n",
    "        lines.append(\"Outcome: Your claim has been accepted. Amount may differ due to deductions or sub-limits.\")\n",
    "    elif decision == \"Rejected\":\n",
    "        lines.append(\"Outcome: Claim not approved. Please review reason and contact insurer if clarification needed.\")\n",
    "    else:\n",
    "        lines.append(\"Outcome: Decision unclear. Please verify with insurer or upload the original PDF for manual review.\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Gen AI: translation + long-text summarization\n",
    "# ------------------------------------------------------------\n",
    "def auto_detect_language(text: str) -> str:\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except Exception:\n",
    "        return \"en\"\n",
    "\n",
    "# Simple in-memory cache for translation models so we don't reload same pair repeatedly\n",
    "_translation_models = {}\n",
    "\n",
    "def get_translation_model(src_lang: str, tgt_lang: str):\n",
    "    key = f\"{src_lang}-{tgt_lang}\"\n",
    "    if key in _translation_models:\n",
    "        return _translation_models[key]\n",
    "    model_name = f\"Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}\"\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    _translation_models[key] = (tokenizer, model)\n",
    "    return tokenizer, model\n",
    "\n",
    "def translate_text(text: str, src_lang: str, tgt_lang: str, max_chunk_chars: int = 1500) -> str:\n",
    "    \"\"\"\n",
    "    Generic translation using MarianMT with chunking for long texts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer, model = get_translation_model(src_lang, tgt_lang)\n",
    "    except Exception as e:\n",
    "        print(f\"Translation model for {src_lang}->{tgt_lang} not found ({e}). Using original text.\")\n",
    "        return text\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(start + max_chunk_chars, n)\n",
    "        split = text.rfind(\".\", start, end)\n",
    "        if split == -1 or split <= start + 100:\n",
    "            split = end\n",
    "        else:\n",
    "            split += 1\n",
    "        chunks.append(text[start:split].strip())\n",
    "        start = split\n",
    "\n",
    "    translated_chunks = []\n",
    "    for c in chunks:\n",
    "        if not c.strip():\n",
    "            continue\n",
    "        inputs = tokenizer(\n",
    "            c,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "        )\n",
    "        out_ids = model.generate(**inputs, max_length=512)\n",
    "        translated = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "        translated_chunks.append(translated.strip())\n",
    "\n",
    "    return \" \".join(translated_chunks).strip()\n",
    "\n",
    "def _summarize_chunk(chunk: str, max_length: int = 180, min_length: int = 50) -> str:\n",
    "    if not chunk.strip():\n",
    "        return \"\"\n",
    "    result = summarizer(\n",
    "        chunk,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    return result[0][\"summary_text\"].strip()\n",
    "\n",
    "def summarize_long_text(\n",
    "    text: str,\n",
    "    max_chunk_chars: int = 2500,\n",
    "    chunk_summary_max_len: int = 180,\n",
    "    chunk_summary_min_len: int = 50,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Chunk long reports, summarize each piece, then optionally summarize summaries.\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return \"‚ö†Ô∏è No text detected in the document.\"\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "\n",
    "    if len(text) <= max_chunk_chars:\n",
    "        return _summarize_chunk(\n",
    "            text,\n",
    "            max_length=chunk_summary_max_len,\n",
    "            min_length=chunk_summary_min_len,\n",
    "        )\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(start + max_chunk_chars, n)\n",
    "        split = text.rfind(\".\", start, end)\n",
    "        if split == -1 or split <= start + 400:\n",
    "            split = end\n",
    "        else:\n",
    "            split += 1\n",
    "        chunk = text[start:split].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start = split\n",
    "\n",
    "    partial_summaries = []\n",
    "    for i, c in enumerate(chunks, start=1):\n",
    "        # No emoji here to avoid UnicodeEncodeError on Windows console\n",
    "        print(f\"Summarizing chunk {i}/{len(chunks)} (len={len(c)} chars)\")\n",
    "        if len(c.split()) < 40:\n",
    "            partial_summaries.append(c)\n",
    "        else:\n",
    "            s = _summarize_chunk(\n",
    "                c,\n",
    "                max_length=chunk_summary_max_len,\n",
    "                min_length=chunk_summary_min_len,\n",
    "            )\n",
    "            partial_summaries.append(s)\n",
    "\n",
    "    combined = \" \".join(partial_summaries)\n",
    "    combined = re.sub(r\"\\s+\", \" \", combined).strip()\n",
    "\n",
    "    if len(combined) <= max_chunk_chars:\n",
    "        final = _summarize_chunk(\n",
    "            combined,\n",
    "            max_length=chunk_summary_max_len,\n",
    "            min_length=chunk_summary_min_len,\n",
    "        )\n",
    "        return final\n",
    "\n",
    "    return combined\n",
    "\n",
    "def generate_claim_summary(raw_text: str, back_translate: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    High-level NLP summary:\n",
    "    - Detect language\n",
    "    - Translate to English if needed\n",
    "    - Summarize long text\n",
    "    - Optionally translate back to source language\n",
    "    \"\"\"\n",
    "    if not raw_text or not raw_text.strip():\n",
    "        return \"‚ö†Ô∏è No text detected to summarize.\"\n",
    "\n",
    "    src_lang = auto_detect_language(raw_text)\n",
    "    # No emoji in console print (Windows cp1252 issue)\n",
    "    print(f\"Detected language for summary: {src_lang}\")\n",
    "\n",
    "    if src_lang != \"en\":\n",
    "        text_en = translate_text(raw_text, src_lang, \"en\")\n",
    "    else:\n",
    "        text_en = raw_text\n",
    "\n",
    "    summary_en = summarize_long_text(text_en)\n",
    "\n",
    "    if back_translate and src_lang != \"en\":\n",
    "        final_summary = translate_text(summary_en, \"en\", src_lang)\n",
    "    else:\n",
    "        final_summary = summary_en\n",
    "\n",
    "    return final_summary.strip()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# File text extraction (pdf / image / txt)\n",
    "# ------------------------------------------------------------\n",
    "def extract_text(file) -> str:\n",
    "    suffix = Path(file.name).suffix.lower()\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)\n",
    "    tmp.write(file.read()); tmp.flush(); tmp.close()\n",
    "    text = \"\"\n",
    "    try:\n",
    "        if suffix == \".pdf\":\n",
    "            with pdfplumber.open(tmp.name) as pdf:\n",
    "                for p in pdf.pages:\n",
    "                    page_text = p.extract_text() or \"\"\n",
    "                    text += page_text + \"\\n\"\n",
    "        elif suffix in (\".jpg\", \".jpeg\", \".png\"):\n",
    "            if ocr_model is None:\n",
    "                raise RuntimeError(\"OCR model not available in this environment.\")\n",
    "            doc = DocumentFile.from_images(tmp.name)\n",
    "            res = ocr_model(doc)\n",
    "            try:\n",
    "                for pg in res.pages:\n",
    "                    try:\n",
    "                        text += pg.get_text() + \"\\n\"\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if not text.strip():\n",
    "                    text = res.render()\n",
    "            except Exception:\n",
    "                text = res.render()\n",
    "        elif suffix == \".txt\":\n",
    "            with open(tmp.name, encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "                text = fh.read()\n",
    "        else:\n",
    "            with open(tmp.name, encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "                text = fh.read()\n",
    "    finally:\n",
    "        try:\n",
    "            os.unlink(tmp.name)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return clean_text(text)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Streamlit UI\n",
    "# ------------------------------------------------------------\n",
    "with st.container():\n",
    "    st.markdown('<div class=\"card\">', unsafe_allow_html=True)\n",
    "    file = st.file_uploader(\"Upload claim report (.pdf, .jpg, .jpeg, .png, .txt)\", type=[\"pdf\", \"jpg\", \"jpeg\", \"png\", \"txt\"])\n",
    "    go = st.button(\"‚ö° Summarize Claim\", type=\"primary\", disabled=(file is None))\n",
    "    st.markdown('</div>', unsafe_allow_html=True)\n",
    "\n",
    "if go and file:\n",
    "    with st.spinner(\"Processing and summarizing‚Ä¶\"):\n",
    "        try:\n",
    "            raw_text = extract_text(file)\n",
    "        except Exception as e:\n",
    "            st.error(f\"Failed to extract text: {e}\")\n",
    "            raw_text = \"\"\n",
    "\n",
    "        info = extract_info(raw_text)\n",
    "        structured_summary = quick_summary(raw_text, info)\n",
    "        nlp_summary = generate_claim_summary(raw_text, back_translate=True)\n",
    "\n",
    "    # ---------- Display results ----------\n",
    "    st.markdown('<div class=\"result\">', unsafe_allow_html=True)\n",
    "\n",
    "    st.markdown('<div class=\"section-title\">üìå Structured Explanation (Rule-Based)</div>', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"section-sub\">Extracted using regex patterns and decision rules.</div>', unsafe_allow_html=True)\n",
    "    st.text(structured_summary)\n",
    "\n",
    "    st.markdown('<div class=\"section-title\">ü§ñ AI-Generated Summary (NLP Model)</div>', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"section-sub\">Generated using transformers with long-text handling.</div>', unsafe_allow_html=True)\n",
    "    st.text(nlp_summary)\n",
    "\n",
    "    st.markdown('</div>', unsafe_allow_html=True)\n",
    "\n",
    "    combined_for_download = (\n",
    "        \"Structured Explanation (Rule-Based):\\n\"\n",
    "        + structured_summary\n",
    "        + \"\\n\\n\"\n",
    "        + \"AI-Generated Summary (NLP Model):\\n\"\n",
    "        + nlp_summary\n",
    "    )\n",
    "    st.download_button(\n",
    "        \"‚¨áÔ∏è Download Explanation\",\n",
    "        data=combined_for_download.encode('utf-8'),\n",
    "        file_name=\"claim_summary.txt\",\n",
    "        mime=\"text/plain\",\n",
    "    )\n",
    "\n",
    "st.markdown('<div class=\"footer\">Simple. Fast. Understandable ‚Äî instant claim explanations.</div>', unsafe_allow_html=True)\n",
    "\n",
    "# ============================================================\n",
    "# End of file\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb8df13-6168-4f3a-b2f4-c10a0e3e75c6",
   "metadata": {},
   "outputs": [],
   "source": [
    " !streamlit run claim_explainer_app_fixed.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba63da-38e1-4b46-9ca0-0b15bae12545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
